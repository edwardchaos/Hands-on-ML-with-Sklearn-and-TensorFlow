{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly many weak learners (one that is a little better than random) voting on a class performs in many cases better than a strong learner.\n",
    "\n",
    "This is because of the law of large numbers. For example, imagine tossing a coin that lands on heads 51% of the time and 49% on tails. After tossing the coin 1000 times, we'd expect about 510 heads and 490 tails. It turns out with 1000 tosses, there's a 75% chance of guessing heads. With 10000 tosses, we're at 97% chance of guessing heads.\n",
    "\n",
    "With an infinite number of tosses, the ratio approaches the exact probability distribution of the coin which is 51% heads 49% tails. In this case, the ensemble of weak learners will always predict heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 2)\n",
      "dict_keys(['DESCR', 'data', 'feature_names', 'target_names', 'target'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('rnd', Random...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:,2:] #Only care about last 2 features\n",
    "print(X.shape)\n",
    "print(iris.keys())\n",
    "y = iris[\"target\"].astype(np.float64)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,\n",
    "                                                   random_state=42)\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[(\"lr\", log_clf),(\"rnd\", rnd_clf),(\"svm\", svm_clf)],\n",
    "    voting=\"hard\"\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.8666666666666667\n",
      "RandomForestClassifier 1.0\n",
      "SVC 1.0\n",
      "VotingClassifier 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edwardchaos/.local/share/virtualenvs/TF_py3/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test,y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging and Pasting are methods where instead of training different types of classifiers, we train an ensemble of same type classifiers but with different subsets of data.\n",
    "\n",
    "Bagging: Sampling with replacement\n",
    "Pasting: Sampling without replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators=500,\n",
    "    max_samples=100,\n",
    "    bootstrap=True, #With replacement\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "bag_clf.fit(X_train,y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-bag Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With bagging, since replacement is allowed, there may be instances that are sampled multiple times and others that are not sampled at all. The ones that are not sampled are called \"out of bag\" (oob) instances. Since the classifier never sees these instances, they can be used in validating performance.\n",
    "\n",
    "In this way, the effect of cross-validation is free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators=500,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    oob_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9416666666666667"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train,y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.06761006, 0.93238994],\n",
       "       [0.        , 0.99481865, 0.00518135],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.00505051, 0.99494949],\n",
       "       [0.        , 0.90449438, 0.09550562],\n",
       "       [0.        , 0.99418605, 0.00581395],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.44632768, 0.55367232],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.99470899, 0.00529101],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.24873056, 0.75126944],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.02837838, 0.97162162],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.49774306, 0.50225694],\n",
       "       [0.        , 0.0060241 , 0.9939759 ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.99468085, 0.00531915],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.41326531, 0.58673469],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.04790419, 0.95209581],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.13333333, 0.86666667],\n",
       "       [0.        , 0.13265306, 0.86734694],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.8547486 , 0.1452514 ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.68041237, 0.31958763],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.99425287, 0.00574713],\n",
       "       [0.        , 0.51392535, 0.48607465],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.99470899, 0.00529101],\n",
       "       [0.        , 0.00578035, 0.99421965],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.56424581, 0.43575419],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.68062827, 0.31937173],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.11107955, 0.88892045],\n",
       "       [0.        , 0.04481589, 0.95518411],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.96882022, 0.03117978],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Patches and Random Subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of boostrapping on training instances, we can instead boostrap on individual features. This is particularly useful in datasets with high dimensionaly such as images. \n",
    "\n",
    "Bootstrapping on instances AND features is called \"Random Patches\"\n",
    "Bootstrapping only on features is called \"Random Subspaces\"\n",
    "\n",
    "\n",
    "Bootstrapping BIAS vs. VARIANCE tradeoff:\n",
    "- Boot strapping increases bias and reduces variance\n",
    "    - Since each classifier has less data to work with, it \"underfits\" more hence increased bias.\n",
    "    - All classifiers in the ensemble in a way have slightly different decision boundaries that constrains the solution space in slightly differnet ways hence decreases the variance of the ensemble as a whole\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier is a customized BaggingClassifier just for decision trees.\n",
    "\n",
    "Instead of splitting greedily on the next best feature when growing a tree, RandomForest by default chooses the best split from a set of randomly selected features. This in turn reduces correlation between individual decision trees thereby further increasing bias and reducing variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500,\n",
    "                                max_leaf_nodes=16,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "rnd_clf.fit(X_train,y_train)\n",
    "y_pred = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra-Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extremely Randomized Trees further increase bias and reduce variance.\n",
    "\n",
    "Instead of any sort of greedy selection while growing a tree, both attribute AND tolerance are selected randomly.\n",
    "\n",
    "This is the epitome of \"weak learners\"\n",
    "\n",
    "This is faster to train, so could probably train more weak learners and hope on the law of large numbers to carry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09665730257203743\n",
      "sepal width (cm) 0.022836607349913437\n",
      "petal length (cm) 0.4313242460751427\n",
      "petal width (cm) 0.4491818440029059\n"
     ]
    }
   ],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=500,n_jobs=-1)\n",
    "iris = load_iris()\n",
    "rnd_clf.fit(iris['data'], iris['target'])\n",
    "\n",
    "#Display importance of features\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name,score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeaway: Random Forests are a great way to quickly get a sense of which features are the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training classifiers sequentially, each improving on the previous.\n",
    "\n",
    "Does not scale as well as Bagging since each predictor relies on the training of its predecessor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost focuses on trainig examples that were predicted incorreclty. Potentially the most popular boosting strategy, AdaBoost works by setting equal weights for all training instances initially. The weights for incorrectly classified instances are increased so that the next classifier would focus more on these. \n",
    "\n",
    "Typically AdaBoost is an ensemble of \"decision stumps\" (decision trees with depth=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "          max_features='auto', max_leaf_nodes=None,\n",
       "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "          min_samples_leaf=1, min_samples_split=2,\n",
       "          min_weight_fraction_leaf=0.0, random_state=None,\n",
       "          splitter='random'),\n",
       "          learning_rate=0.5, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    ExtraTreeClassifier(max_depth=1),\n",
    "    n_estimators=200,\n",
    "    algorithm=\"SAMME.R\",\n",
    "    learning_rate=0.5\n",
    ")\n",
    "\n",
    "ada_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of modifying instance weights as with AdaBoost, Gradient Boost optimizes incrementally on the residual errors.\n",
    "\n",
    "A.K.A.\n",
    "Gradient Tree Boosting\n",
    "Gradient Boosted Regression Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A step by step demonstration of how gradient boosting works\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y-tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X,y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(X) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sklearn's GradientBoostingRegressor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=1.0, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=3, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2,\n",
    "                                 n_estimators=3,\n",
    "                                 learning_rate=1.0)\n",
    "\n",
    "gbrt.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning_rate parameter is a hyperparameter that controls how much each tree's predictions contribute towards the ensemble prediction.\n",
    "\n",
    "A lower value requires more trees in the ensemble but also generally improves generalisability.\n",
    "\n",
    "One way of selecting the number of trees is with early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train,X_val,y_train,y_val = train_test_split(X,y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2,n_estimators=120)\n",
    "gbrt.fit(X_train,y_train)\n",
    "\n",
    "errors=[mean_squared_error(y_val, y_pred)\n",
    "           for y_pred in gbrt.staged_predict(X_val)]\n",
    "\n",
    "best_numberof_estimators = np.argmin(errors)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=best_numberof_estimators)\n",
    "gbrt_best.fit(X_train,y_train)\n",
    "print(len(gbrt_best.estimators_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f30d4d13710>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_estimators = len(errors)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(num_estimators), errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping without having to compute trees up to high num of learners. Instead, use \"warm_start\" to continue where it last left off everytime you call fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "\n",
    "min_val_error=float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1,120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train,y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val,y_pred)\n",
    "    \n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            #To account for noise, we don't want to stop as soon as\n",
    "            #Error goes up once.\n",
    "            break; #Stop early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "print(len(gbrt.estimators_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking (Stacked Generalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ensemble methods we've discussed previously, the scores of individual predictors are aggregated as one by means of statistical mode, or weighted based on prediction probability.\n",
    "\n",
    "Stacking abstracts this step away from the user by embedding this aggregation into the model. \n",
    "\n",
    "Simplest explaination (Ensemble of 3 predictors):\n",
    "\n",
    "Step 1: Split training set into 2 parts. One for the usual training of predictors and one for training the \"blender\".\n",
    "\n",
    "Step 2: Use the first training set to train the predictors as usual. This sets the weights of the predictors accordingly.\n",
    "\n",
    "Step 3: Use the second set as inputs to the trained predictors to generate 3 outputs. By using examples the predictors haven't seen before, we get \"clean\" outputs so they're not overfitted.\n",
    "\n",
    "Step 4: Use the 3 outputs to train a \"blender\" that outputs a single class/value. A blender can be any type (Linear Regressor, Random Forest Regression, etc.)\n",
    "\n",
    "Step 5: Finally, the output of the blender is the overall predicted class.\n",
    "\n",
    "Scikit learn doesn't provide Stacks directly but can be built with individual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
